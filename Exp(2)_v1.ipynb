{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "uncomment for google colab"
      ],
      "metadata": {
        "id": "-YFjlec-VeG9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "!pip install pingouin==0.5.5\n",
        "from google.colab import files\n",
        "files.upload()\n",
        "\"\"\""
      ],
      "metadata": {
        "id": "XoWZumXCVOQn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Import packages**"
      ],
      "metadata": {
        "id": "LyrYKZ2Pdpeu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import ipywidgets as widgets\n",
        "from IPython.display import display, clear_output, HTML\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import os\n",
        "import statsmodels.api as sm\n",
        "from statsmodels.formula.api import mixedlm\n",
        "import warnings\n",
        "import pingouin as pg\n",
        "import matplotlib.pyplot as plt\n",
        "from matplotlib.colors import ListedColormap\n",
        "import matplotlib.patches as mpatches\n",
        "import colorcet as cc\n",
        "from scipy.stats import spearmanr, binomtest, ttest_1samp\n",
        "from scipy.optimize import curve_fit\n",
        "import networkx as nx\n",
        "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
        "from sklearn.feature_selection import f_classif, SelectKBest\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.model_selection import StratifiedKFold, GroupKFold, KFold, train_test_split\n",
        "from sklearn.metrics import confusion_matrix\n",
        "from sklearn.manifold import TSNE\n",
        "from sklearn.decomposition import PCA\n",
        "from sklearn.cluster import KMeans\n",
        "from sklearn.gaussian_process import GaussianProcessRegressor\n",
        "from sklearn.gaussian_process.kernels import WhiteKernel, Matern,RBF\n",
        "import umap\n",
        "import json\n",
        "import torch\n",
        "import torch.nn as nn"
      ],
      "metadata": {
        "id": "5zm683bStydw"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Neural Networks**"
      ],
      "metadata": {
        "id": "quc1gqH_und0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Generator(nn.Module):\n",
        "    def __init__(self, input_dim, output_dim, hidden_dim, noise_dim=1):\n",
        "        super().__init__()\n",
        "        self.net = nn.Sequential(\n",
        "            nn.Linear(input_dim + noise_dim, hidden_dim),\n",
        "            nn.Tanh(),\n",
        "            nn.Linear(hidden_dim, output_dim),\n",
        "            nn.Softplus()\n",
        "        )\n",
        "    def forward(self, x, z):\n",
        "        xz = torch.cat([x, z], dim=1)\n",
        "        return self.net(xz)\n",
        "\n",
        "class Discriminator(nn.Module):\n",
        "    def __init__(self, input_dim, output_dim, hidden_dim):\n",
        "        super().__init__()\n",
        "        self.net = nn.Sequential(\n",
        "            nn.Linear(input_dim + output_dim, hidden_dim),\n",
        "            nn.Tanh(),\n",
        "            nn.Linear(hidden_dim, 1),\n",
        "            nn.Sigmoid()\n",
        "        )\n",
        "    def forward(self, x, y):\n",
        "        xy = torch.cat([x, y], dim=1)\n",
        "        return self.net(xy)\n",
        "\n",
        "class cGAN:\n",
        "    def __init__(self, seed, noise_dim=1, hidden_dim=256, max_epochs=1000, patience=10):\n",
        "        self.noise_dim = noise_dim\n",
        "        self.hidden_dim = hidden_dim\n",
        "        self.max_epochs = max_epochs\n",
        "        self.gen = None\n",
        "        self.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "        self.seed = seed\n",
        "        self.patience = patience\n",
        "    def fit(self, X, y):\n",
        "        X = np.asarray(X).astype(np.float32)\n",
        "        y = np.asarray(y).astype(np.float32)\n",
        "        X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=self.seed)\n",
        "        X_train = torch.tensor(X_train).to(self.device)\n",
        "        y_train = torch.tensor(y_train).to(self.device)\n",
        "        X_val = torch.tensor(X_val).to(self.device)\n",
        "        y_val = torch.tensor(y_val).to(self.device)\n",
        "        input_dim = X.shape[1]\n",
        "        output_dim = y.shape[1]\n",
        "        batch_size = len(X_train)\n",
        "        G = Generator(input_dim, output_dim, self.hidden_dim).to(self.device)\n",
        "        D = Discriminator(input_dim, output_dim, self.hidden_dim).to(self.device)\n",
        "        optimizer_G = torch.optim.Adam(G.parameters(), lr=0.001)\n",
        "        optimizer_D = torch.optim.Adam(D.parameters(), lr=0.001)\n",
        "        criterion = nn.BCELoss()\n",
        "        best_val_loss = float('inf')\n",
        "        wait = 0\n",
        "        for epoch in range(self.max_epochs):\n",
        "            G.train()\n",
        "            D.train()\n",
        "            real_labels = torch.ones(batch_size, 1).to(self.device)\n",
        "            fake_labels = torch.zeros(batch_size, 1).to(self.device)\n",
        "            z = torch.randn(batch_size, self.noise_dim).to(self.device)\n",
        "            y_fake = G(X_train, z)\n",
        "            D_real = D(X_train, y_train)\n",
        "            D_fake = D(X_train, y_fake)\n",
        "            D.train()\n",
        "            G.eval()\n",
        "            optimizer_D.zero_grad()\n",
        "            z = torch.randn(batch_size, self.noise_dim).to(self.device)\n",
        "            y_fake = G(X_train, z).detach()\n",
        "            D_real = D(X_train, y_train)\n",
        "            D_fake = D(X_train, y_fake)\n",
        "            loss_D = criterion(D_real, real_labels) + criterion(D_fake, fake_labels)\n",
        "            loss_D.backward()\n",
        "            optimizer_D.step()\n",
        "            G.train()\n",
        "            D.eval()\n",
        "            optimizer_G.zero_grad()\n",
        "            z = torch.randn(batch_size, self.noise_dim).to(self.device)\n",
        "            y_fake = G(X_train, z)\n",
        "            D_fake = D(X_train, y_fake)\n",
        "            loss_G = criterion(D_fake, real_labels)\n",
        "            loss_G.backward()\n",
        "            optimizer_G.step()\n",
        "            G.eval()\n",
        "            with torch.no_grad():\n",
        "                z_val = torch.randn(len(X_val), self.noise_dim).to(self.device)\n",
        "                y_val_pred = G(X_val, z_val)\n",
        "                val_loss = torch.mean((y_val_pred - y_val) ** 2).item()\n",
        "            if val_loss < best_val_loss:\n",
        "                best_val_loss = val_loss\n",
        "                wait = 0\n",
        "                self.gen = G\n",
        "            else:\n",
        "                wait += 1\n",
        "                if wait >= self.patience:\n",
        "                    print(f\"Early stopping at epoch {epoch}\")\n",
        "                    break\n",
        "        print(f\"loss_D = {loss_D.item():.4f}, loss_G = {loss_G.item():.4f}\")\n",
        "    def predict(self, X):\n",
        "        self.gen.eval()\n",
        "        X = np.asarray(X).astype(np.float32)\n",
        "        X = torch.tensor(X).to(self.device)\n",
        "        torch.manual_seed(self.seed)\n",
        "        z = torch.randn(len(X), self.noise_dim).to(self.device)\n",
        "        with torch.no_grad():\n",
        "            y_pred = self.gen(X, z).cpu().numpy()\n",
        "        return y_pred\n",
        "class Encoder(nn.Module):\n",
        "    def __init__(self, input_dim, output_dim, hidden_dim, latent_dim):\n",
        "        super().__init__()\n",
        "        self.fc = nn.Sequential(\n",
        "            nn.Linear(input_dim + output_dim, hidden_dim),\n",
        "            nn.Tanh()\n",
        "        )\n",
        "        self.fc_mu = nn.Linear(hidden_dim, latent_dim)\n",
        "        self.fc_logvar = nn.Linear(hidden_dim, latent_dim)\n",
        "\n",
        "    def forward(self, x, y):\n",
        "        xy = torch.cat([x, y], dim=1)\n",
        "        h = self.fc(xy)\n",
        "        mu = self.fc_mu(h)\n",
        "        logvar = self.fc_logvar(h)\n",
        "        return mu, logvar\n",
        "class Decoder(nn.Module):\n",
        "    def __init__(self, input_dim, output_dim, hidden_dim, latent_dim):\n",
        "        super().__init__()\n",
        "        self.net = nn.Sequential(\n",
        "            nn.Linear(input_dim + latent_dim, hidden_dim),\n",
        "            nn.Tanh(),\n",
        "            nn.Linear(hidden_dim, output_dim)\n",
        "        )\n",
        "    def forward(self, x, z):\n",
        "        xz = torch.cat([x, z], dim=1)\n",
        "        return self.net(xz)\n",
        "class CVAE:\n",
        "    def __init__(self, seed, latent_dim=1, hidden_dim=256, max_epochs=1000, patience=10):\n",
        "        self.latent_dim = latent_dim\n",
        "        self.hidden_dim = hidden_dim\n",
        "        self.max_epochs = max_epochs\n",
        "        self.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "        self.seed = seed\n",
        "        self.patience = patience\n",
        "        self.decoder = None\n",
        "    def reparameterize(self, mu, logvar):\n",
        "        std = torch.exp(0.5 * logvar)\n",
        "        eps = torch.randn_like(std)\n",
        "        return mu + eps * std\n",
        "    def fit(self, X, y):\n",
        "        torch.manual_seed(self.seed)\n",
        "        X = np.asarray(X).astype(np.float32)\n",
        "        y = np.asarray(y).astype(np.float32)\n",
        "        X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=self.seed)\n",
        "        X_train = torch.tensor(X_train).to(self.device)\n",
        "        y_train = torch.tensor(y_train).to(self.device)\n",
        "        X_val = torch.tensor(X_val).to(self.device)\n",
        "        y_val = torch.tensor(y_val).to(self.device)\n",
        "        input_dim = X.shape[1]\n",
        "        output_dim = y.shape[1]\n",
        "        batch_size = len(X_train)\n",
        "        encoder = Encoder(input_dim, output_dim, self.hidden_dim, self.latent_dim).to(self.device)\n",
        "        decoder = Decoder(input_dim, output_dim, self.hidden_dim, self.latent_dim).to(self.device)\n",
        "        optimizer = torch.optim.Adam(list(encoder.parameters()) + list(decoder.parameters()), lr=0.001)\n",
        "        best_val_loss = float('inf')\n",
        "        wait = 0\n",
        "        for epoch in range(self.max_epochs):\n",
        "            encoder.train()\n",
        "            decoder.train()\n",
        "            mu, logvar = encoder(X_train, y_train)\n",
        "            z = self.reparameterize(mu, logvar)\n",
        "            y_pred = decoder(X_train, z)\n",
        "            recon_loss = nn.MSELoss()(y_pred, y_train)\n",
        "            kl_loss = -0.5 * torch.mean(1 + logvar - mu.pow(2) - logvar.exp())\n",
        "            loss = recon_loss + kl_loss\n",
        "            optimizer.zero_grad()\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "            encoder.eval()\n",
        "            decoder.eval()\n",
        "            with torch.no_grad():\n",
        "                mu_val, logvar_val = encoder(X_val, y_val)\n",
        "                z_val = self.reparameterize(mu_val, logvar_val)\n",
        "                y_val_pred = decoder(X_val, z_val)\n",
        "                val_loss = nn.MSELoss()(y_val_pred, y_val).item()\n",
        "            if val_loss < best_val_loss:\n",
        "                best_val_loss = val_loss\n",
        "                wait = 0\n",
        "                self.decoder = decoder\n",
        "            else:\n",
        "                wait += 1\n",
        "                if wait >= self.patience:\n",
        "                    print(f\"Early stopping at epoch {epoch}\")\n",
        "                    break\n",
        "        print(f\"Train Loss = {loss.item():.4f}, Val Loss = {val_loss:.4f}\")\n",
        "    def predict(self, X):\n",
        "        X = torch.tensor(np.asarray(X).astype(np.float32)).to(self.device)\n",
        "        self.decoder.eval()\n",
        "        with torch.no_grad():\n",
        "            z = torch.randn(len(X), self.latent_dim).to(self.device)\n",
        "            y_pred = self.decoder(X, z)\n",
        "        return y_pred.cpu().numpy()"
      ],
      "metadata": {
        "id": "zEalqvDCurSz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**All functions called in the tool**"
      ],
      "metadata": {
        "id": "K_-epxCIdv8g"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "######\n",
        "def data_extraction(b):\n",
        "  with extraction_output:\n",
        "        clear_output()\n",
        "        try:\n",
        "          print(\"Running data extraction...\")\n",
        "          Outliers = list(map(int, outliers_input.value.split(',')))\n",
        "          Flag_unannotated = include_unanot_checkbox.value\n",
        "          if Flag_unannotated:\n",
        "              Unannotated_neg = pd.read_excel(\"S3WP_LC_unannotaed_neg_final.xlsx\")\n",
        "              Unannotated_pos = pd.read_excel(\"S3WP_LC_unannotaed_pos_final.xlsx\")\n",
        "              list_neg, list_pos = list(Unannotated_neg), list(Unannotated_pos)\n",
        "              Unannotated_neg = Unannotated_neg.to_numpy()\n",
        "              Unannotated_pos = Unannotated_pos.to_numpy()\n",
        "              Data_neg = []\n",
        "              Data_pos = []\n",
        "              mislabel = []\n",
        "          Metadata = pd.read_excel(\"S3WP_LC_samples_6_visits.xlsx\")\n",
        "          Keys_meta = Metadata[[\"Subject\", \"Visit\"]].to_numpy()\n",
        "          Keys_meta = list(zip(Keys_meta[:,0], Keys_meta[:,1]))\n",
        "          Data_annotated = pd.read_excel(\"S3WP_annotated_Levels_1_2_pos&neg.xlsx\")\n",
        "          Features_annotated = Data_annotated.to_numpy()\n",
        "          Keys_annotated = list(zip(Features_annotated[528,:], Features_annotated[529,:]))\n",
        "          Data_full = []\n",
        "          Labels_full = []\n",
        "          timepoints_full = []\n",
        "          participants_full = []\n",
        "          for i in range(np.shape(Keys_meta)[0]):\n",
        "              x = Keys_annotated.index(Keys_meta[i])\n",
        "              dim = 519 #annotated features\n",
        "              data_line = Features_annotated[:dim, x].reshape(1, -1)\n",
        "              label_line = Features_annotated[dim:, x].reshape(1, -1)\n",
        "              Data_full = data_line if np.size(Data_full) == 0 else np.concatenate([Data_full, data_line])\n",
        "              Labels_full = label_line if np.size(Labels_full) == 0 else np.concatenate([Labels_full, label_line])\n",
        "              t_i = Keys_meta[i][1]\n",
        "              p_i = Keys_meta[i][0]\n",
        "              timepoints_full = np.append(timepoints_full, t_i)\n",
        "              participants_full = np.append(participants_full, p_i)\n",
        "              string_of_sample = f\"{p_i}_v_{t_i}\"\n",
        "              if Flag_unannotated:\n",
        "                  ind_neg = [j for j, s in enumerate(list_neg) if string_of_sample in s][0]\n",
        "                  ind_pos = [j for j, s in enumerate(list_pos) if string_of_sample in s][0]\n",
        "                  line_neg = Unannotated_neg[:, ind_neg].reshape(1, -1)\n",
        "                  line_pos = Unannotated_pos[:, ind_pos].reshape(1, -1)\n",
        "                  Data_neg = line_neg if np.size(Data_neg) == 0 else np.concatenate([Data_neg, line_neg])\n",
        "                  Data_pos = line_pos if np.size(Data_pos) == 0 else np.concatenate([Data_pos, line_pos])\n",
        "                  if string_of_sample in [\"3887_v_2\", \"3829_v_2\"]:\n",
        "                      mislabel.append(i)\n",
        "          if Flag_unannotated:\n",
        "              Data_unanot_full = np.concatenate([Data_neg[:, 9:], Data_pos[:, 10:]], axis=1)\n",
        "              Data_unanot_full[mislabel, :] = Data_unanot_full[[mislabel[1], mislabel[0]], :]\n",
        "              Data_unanot_full = np.concatenate([Data_full, Data_unanot_full], axis=1)\n",
        "              Data_unanot = np.delete(Data_unanot_full, Outliers, axis=0)\n",
        "          Labels_full = Labels_full[:,[0,5,7,8, 10, 12]]\n",
        "          Label_names = [\"Age\", \"BMI\", \"Sex\", \"Smoking\", \"Visit\", \"Season\"]\n",
        "          Data, Labels, timepoints, participants = map(\n",
        "              lambda x: np.delete(x, Outliers, axis=0),\n",
        "              [Data_full, Labels_full, timepoints_full, participants_full]\n",
        "          )\n",
        "          Class_info = Features_annotated[:dim, 16:20]\n",
        "          np.save(\"Data\", Data)\n",
        "          np.save(\"Labels\", Labels)\n",
        "          np.save(\"Labels_full\", Labels_full)\n",
        "          np.save(\"Label_names\", Label_names)\n",
        "          np.save(\"Data_full\", Data_full)\n",
        "          np.save(\"Class_info\", Class_info)\n",
        "          np.save(\"timepoints\", timepoints)\n",
        "          np.save(\"timepoints_full\", timepoints_full)\n",
        "          np.save(\"participants\", participants)\n",
        "          np.save(\"participants_full\", participants_full)\n",
        "          np.save(\"Outliers\", Outliers)\n",
        "          if Flag_unannotated:\n",
        "              np.save(\"Data_unanot_full\", Data_unanot_full)\n",
        "              np.save(\"Data_unanot\", Data_unanot)\n",
        "          print(\"Data extraction complete!\")\n",
        "        except Exception as e:\n",
        "            print(\"Error:\",e)\n",
        "######\n",
        "def show_features(b):\n",
        "        with output_table:\n",
        "          clear_output()\n",
        "          try:\n",
        "            Class_info = np.load(\"Class_info.npy\", allow_pickle=True)\n",
        "            df_features = pd.DataFrame(Class_info, columns=[\"Class\", \"Subclass\", \"Category\", \"Name\"])\n",
        "            display(df_features)\n",
        "          except Exception as e:\n",
        "            print(\"Error:\",e)\n",
        "######\n",
        "def calculate_metrics(b):\n",
        "  with metrics_output:\n",
        "    clear_output()\n",
        "    try:\n",
        "      selected_metrics = list(metric_selector.value)\n",
        "      alpha=alpha_input.value\n",
        "      use_unanot = flag_unanot_metric.value\n",
        "      include_tp = flag_tp_metric.value\n",
        "      if include_tp:\n",
        "        timepoints = np.load(\"timepoints.npy\", allow_pickle=True)\n",
        "        unique_timepoints = np.unique(timepoints)\n",
        "        print(\"The list of time points:\", unique_timepoints)\n",
        "        unique_timepoints = np.append(unique_timepoints, 0)\n",
        "      else: unique_timepoints = [0]\n",
        "      filename = \"Data\"\n",
        "      if use_unanot:\n",
        "        filename += \"_unanot\"\n",
        "      Data = np.load(filename+\".npy\", allow_pickle=True)\n",
        "      dim = Data.shape[1]\n",
        "      print(\"dimensionality is\", dim)\n",
        "      for tp in unique_timepoints:\n",
        "        tp_str = tp if tp>0 else \"All\"\n",
        "        print(f\"Metrics for {tp_str} tp:\")\n",
        "        if tp == 0:\n",
        "          Data_t = Data\n",
        "        else:\n",
        "          Data_t = Data[timepoints == tp, :]\n",
        "        for Metric in selected_metrics:\n",
        "            if Metric == \"Correlation\":\n",
        "              print(f\"{Metric} is calculating\")\n",
        "              Results = np.zeros((dim,dim))\n",
        "              with warnings.catch_warnings(action=\"ignore\"):\n",
        "                for i in range(1,dim):\n",
        "                    for j in range(i):\n",
        "                        corr, p_value = spearmanr(Data_t[:, i], Data_t[:, j])\n",
        "                        corr = abs(corr)\n",
        "                        if  p_value < alpha:\n",
        "                            Results[i,j] = corr\n",
        "              np.save(f\"{Metric}_{int(tp)}\", Results)\n",
        "              print(f\"{Metric} is saved\")\n",
        "            elif Metric == \"Probability\":\n",
        "              print(f\"{Metric} is calculating\")\n",
        "              Results = np.zeros((dim,dim))\n",
        "              Bonferroni = alpha/2\n",
        "              Binary = (Data_t != 0).astype(int)\n",
        "              frequency = np.sum(Binary, axis=0) / np.shape(Binary)[0]\n",
        "              sparsity_index = [i for i in range(len(frequency)) if frequency[i] < 1]\n",
        "              for k in sparsity_index[1:]:\n",
        "                X_k = Binary[:, k]\n",
        "                for q in [i for i in sparsity_index if i<k]:\n",
        "                  X_q = Binary[:, q]\n",
        "                  N = np.dot(X_q,X_k)\n",
        "                  Dq = np.sum(X_q)\n",
        "                  Dk = np.sum(X_k)\n",
        "                  pvalq = 1 if N==0 else binomtest(N, Dq, 1/2, alternative='greater').pvalue\n",
        "                  pvalk = 1 if N==0 else binomtest(N, Dk, 1 / 2, alternative='greater').pvalue\n",
        "                  if pvalq<Bonferroni and pvalk < Bonferroni:\n",
        "                      Results[k, q]=N/Dq/2+ N / Dk/2\n",
        "              np.save(f\"{Metric}_{int(tp)}\", Results)\n",
        "              print(f\"{Metric} is saved\")\n",
        "            elif Metric == \"Logistic\":\n",
        "              print(f\"{Metric} is calculating\")\n",
        "              Results = np.zeros((dim,dim))\n",
        "              with warnings.catch_warnings(action=\"ignore\"):\n",
        "                Bonferroni = alpha/2\n",
        "                Binary = (Data_t != 0).astype(int)\n",
        "                frequency = np.sum(Binary, axis=0) / np.shape(Binary)[0]\n",
        "                sparsity_index = [i for i in range(len(frequency)) if frequency[i] < 1]\n",
        "                for k in sparsity_index[1:]:\n",
        "                  Y_k = Binary[:, k]\n",
        "                  X_k = pd.DataFrame(Y_k)\n",
        "                  X_k = sm.add_constant(X_k)\n",
        "                  for q in [i for i in sparsity_index if i<k]:\n",
        "                    Y_q = Binary[:, q]\n",
        "                    X_q = pd.DataFrame(Y_q)\n",
        "                    X_q = sm.add_constant(X_q)\n",
        "                    model_1 = sm.Logit(Y_k, X_q)\n",
        "                    result_1 = model_1.fit(method=\"bfgs\",disp=0)\n",
        "                    pvalue_1 = result_1.pvalues[0]\n",
        "                    model_2 = sm.Logit(Y_q, X_k)\n",
        "                    result_2 = model_2.fit(method=\"bfgs\",disp=0)\n",
        "                    pvalue_2 = result_2.pvalues[0]\n",
        "                    if pvalue_1<Bonferroni and pvalue_2 < Bonferroni:\n",
        "                        Results[k, q]=(abs(result_1.params[0])+abs(result_2.params[0]))/2\n",
        "              np.save(f\"{Metric}_{int(tp)}\", Results)\n",
        "              print(f\"{Metric} is saved\")\n",
        "      if \"ICC\" in selected_metrics:\n",
        "        print(\"ICC is calculating\")\n",
        "        Data_full = np.load(filename+\"_full.npy\", allow_pickle=True)\n",
        "        Outliers = np.load(\"Outliers.npy\")\n",
        "        timepoints_full = np.load(\"timepoints_full.npy\")\n",
        "        participants_full = np.load(\"participants_full.npy\")\n",
        "        Outliers_subject = []\n",
        "        for i in Outliers:\n",
        "          Outliers_subject = np.concatenate([Outliers_subject, np.where(participants_full == participants_full[i])[0]]).astype(\"int\")\n",
        "        Data_subject, timepoints_subject, participants_subject = np.delete(Data_full, Outliers_subject, axis=0), np.delete(timepoints_full,Outliers_subject, axis=0), np.delete(participants_full,Outliers_subject, axis=0)\n",
        "        data_icc = np.concatenate([Data_subject, timepoints_subject.reshape(-1,1), participants_subject.reshape(-1,1)], axis=1)\n",
        "        column_names = [f\"Feature_{i}\" for i in range(dim)] + [\"Time\", \"Subject\"]\n",
        "        df_icc = pd.DataFrame(data_icc, columns=column_names)\n",
        "        df_icc = df_icc.apply(pd.to_numeric, errors=\"coerce\")\n",
        "        Results = []\n",
        "        for i in range(dim):\n",
        "            icc = pg.intraclass_corr(\n",
        "                data=df_icc,\n",
        "                targets=\"Subject\",  # Participants\n",
        "                raters=\"Time\",  # Timepoints\n",
        "                ratings=\"Feature_\" + str(i)\n",
        "            )\n",
        "            icc = icc.set_index(\"Type\")\n",
        "            Results.append(icc.loc[\"ICC3\", \"ICC\"])\n",
        "        np.save(\"ICC\", Results)\n",
        "        print(\"ICC is saved\")\n",
        "      if \"DF\" in selected_metrics:\n",
        "        print(\"DF is calculating\")\n",
        "        Data_size = np.shape(Data)[0]\n",
        "        Results = [np.mean(Data[:,i]>0) for i in range(dim)]\n",
        "        np.save(\"DF\", Results)\n",
        "        print(\"DF is saved\")\n",
        "    except Exception as e:\n",
        "            print(\"Error:\",e)\n",
        "######\n",
        "def scatter_button_plot(b):\n",
        "  with scatter_output:\n",
        "    clear_output()\n",
        "    try:\n",
        "      Data = np.load(\"Data.npy\", allow_pickle=True).astype(float)\n",
        "      ind1 = int(feature1_input.value)\n",
        "      ind2 = int(feature2_input.value)\n",
        "      random_color = Color_flag.value\n",
        "      feature1,feature2 = Data[:, ind1], Data[:, ind2]\n",
        "      data_zipped = list(zip(feature1, feature2))\n",
        "      data_zipped.sort()\n",
        "      feature1, feature2= [p[0] for p in data_zipped], [p[1] for p in data_zipped]\n",
        "      law_type = fit_type_selector.value\n",
        "      if law_type == \"Linear\":\n",
        "        coef = np.polyfit(feature1, feature2, 1)\n",
        "        poly1d_fn = np.poly1d(coef)\n",
        "        F = poly1d_fn(feature1)\n",
        "      else:\n",
        "        def power_law(x, a, b):\n",
        "          return a * x**abs(b)\n",
        "        p0 = [1.0, 1.0]\n",
        "        params, cov = curve_fit(power_law, feature1, feature2, p0=p0)\n",
        "        a, b= params\n",
        "        F = power_law(feature1, a, b)\n",
        "      if random_color:\n",
        "        color = '#{:06x}'.format(np.random.randint(0, 0xFFFFFF + 1))\n",
        "      else:\n",
        "        color = \"blue\"\n",
        "      plt.scatter(feature1, feature2, color = color)\n",
        "      plt.plot(feature1, F, '--k')\n",
        "      plt.xlabel(\"Feature \"+str(ind1))\n",
        "      plt.ylabel(\"Feature \"+str(ind2))\n",
        "      #plt.savefig(f\"scatterplot_{ind1}_{ind2}.pdf\", format=\"pdf\", bbox_inches=\"tight\")\n",
        "      plt.show()\n",
        "    except Exception as e:\n",
        "      print(\"Error:\",e)\n",
        "######\n",
        "def show_graph(b):\n",
        "  with graph_output:\n",
        "    clear_output()\n",
        "    try:\n",
        "      corr_value = threshold_input.value\n",
        "      max_connections = number_edges.value\n",
        "      Coloring = Coloring_radio.value\n",
        "      Weight_type = Graph_metric.value\n",
        "      n_tp = Graph_tp.value\n",
        "      random_seed = seed_edges.value\n",
        "      f = lambda x: 2 if x > 10 else 1\n",
        "      if n_tp:\n",
        "        timepoints = np.load(\"timepoints.npy\", allow_pickle=True)\n",
        "        unique_timepoints = np.unique(timepoints)\n",
        "        print(\"The list of time points:\", unique_timepoints)\n",
        "        N_corr = defaultdict(int)\n",
        "        for n_tp in unique_timepoints:\n",
        "            dataname = f\"{Weight_type}_{int(n_tp)}.npy\"\n",
        "            Corr_t = np.load(dataname, allow_pickle=True)\n",
        "            dim = Corr_t.shape[0]\n",
        "            for i in range(1, dim):\n",
        "                for j in range(i):\n",
        "                    corr = Corr_t[i, j]\n",
        "                    if corr > corr_value:\n",
        "                        N_corr[(i, j)] += 1\n",
        "        matching_values= [value for key, value in N_corr.items() if value > 0]\n",
        "        plt.hist(matching_values)\n",
        "        plt.title(\"Histogram for the number of times a pair of features appears in the graph\")\n",
        "        plt.show()\n",
        "        unique_timepoints = np.append(unique_timepoints, 0)\n",
        "      else: unique_timepoints = [0]\n",
        "      for tp in unique_timepoints:\n",
        "        tp_str = int(tp) if tp>0 else \"All\"\n",
        "        if Weight_type == \"Correlation\":\n",
        "          width_coef = 3\n",
        "        elif Weight_type == \"Probability\":\n",
        "          width_coef = 1\n",
        "        elif Weight_type == \"Logistic\":\n",
        "          width_coef = 0.2\n",
        "        Table = np.load(f\"{Weight_type}_{int(tp)}.npy\")\n",
        "        dim = np.shape(Table)[0]\n",
        "        G = nx.Graph()\n",
        "        for i in range(1,dim):\n",
        "          G.add_node(i, label=f\"Feature {i+1}\")\n",
        "          for j in range(i):\n",
        "            corr = Table[i, j]\n",
        "            if corr > corr_value:\n",
        "                G.add_edge(i, j, weight=corr)\n",
        "        nodes_to_remove = [node for node, degree in G.degree() if degree > max_connections] # filter nodes with many connections\n",
        "        G.remove_nodes_from(nodes_to_remove)\n",
        "        components = list(nx.connected_components(G))\n",
        "        filtered_components = [comp for comp in components if len(comp) > 1]\n",
        "        G_filtered = nx.Graph()\n",
        "        for comp in filtered_components:\n",
        "          G_filtered.add_nodes_from(comp)\n",
        "          for node1 in comp:\n",
        "              for node2 in comp:\n",
        "                  if G.has_edge(node1, node2):\n",
        "                      G_filtered.add_edge(node1, node2, weight=G[node1][node2]['weight'])\n",
        "        if Coloring == \"Class\":\n",
        "            Class_info = np.load(\"Class_info.npy\", allow_pickle=True)\n",
        "            Classes = Class_info[:, 0]\n",
        "            unique_classes, class_indices = np.unique(Classes, return_inverse=True)\n",
        "            colors = [\"green\", \"red\",\"deepskyblue\"] + ['#{:06x}'.format(color) for color in np.random.randint(0, 0xFFFFFF + 1, size=100)]\n",
        "        elif Coloring == \"ICC\":\n",
        "            ICC_info = np.load(\"ICC.npy\", allow_pickle=True)\n",
        "            Detection_info = np.load(\"DF.npy\", allow_pickle=True)\n",
        "            freq_thresh = np.percentile(Detection_info, 100/3)\n",
        "            mask_remaining = Detection_info > freq_thresh\n",
        "            icc_median = np.median(ICC_info[mask_remaining])\n",
        "            class_indices = np.empty_like(ICC_info, dtype=int)\n",
        "            class_indices[Detection_info <= freq_thresh] = 0\n",
        "            class_indices[mask_remaining & (ICC_info < icc_median)] = 1\n",
        "            class_indices[mask_remaining & (ICC_info >= icc_median)] = 2\n",
        "            unique_classes = [\"low DF\",\"low ICC\",\"high ICC\"]\n",
        "            colors = [\"#FF6F61\", \"#6B5B95\", \"#88B04B\"]\n",
        "            ICC_graph = [ICC_info[i] for comp in filtered_components for i in comp]\n",
        "            Detection_graph = [Detection_info[i] for comp in filtered_components for i in comp]\n",
        "            print(\"statistics for nodes (mean, std, min, max):\")\n",
        "            print(\"ICC:\",np.round(np.mean(ICC_graph),3), np.round(np.std(ICC_graph),3), np.round(np.min(ICC_graph),3), np.round(np.max(ICC_graph),3))\n",
        "            print(\"DF:\",np.round(np.mean(Detection_graph),3), np.round(np.std(Detection_graph),3), np.round(np.min(Detection_graph),3), np.round(np.max(Detection_graph),3))\n",
        "        Len = len(filtered_components)\n",
        "        sqrt_Len = int(np.sqrt(Len))\n",
        "        plt.figure(figsize=(24, 12))\n",
        "        for i, component in enumerate(filtered_components):\n",
        "          subgraph = G_filtered.subgraph(component)\n",
        "          node_color = [colors[class_indices[node]] for node in subgraph]\n",
        "          pos = nx.spring_layout(subgraph, seed=random_seed, k=1, scale = f(len(component)))\n",
        "          i1, i2 = i%sqrt_Len, i//sqrt_Len\n",
        "          offset_x, offset_y = i1 * 3.5, i2*3.5\n",
        "          for node in pos:\n",
        "              pos[node] = (pos[node][0] + offset_x, pos[node][1] + offset_y)  # Apply offset\n",
        "          edge_weights = [subgraph[u][v]['weight']*width_coef for u, v in subgraph.edges()]\n",
        "          nx.draw(\n",
        "              subgraph, pos, with_labels=True, node_color=node_color, edge_color=\"gray\",\n",
        "              width=edge_weights,node_size=500, font_size=10\n",
        "          )\n",
        "        legend_patches = [mpatches.Patch(color=colors[i], label=unique_classes[i]) for i in range(len(unique_classes))]\n",
        "        plt.legend(handles=legend_patches, loc=\"upper right\", ncol=len(unique_classes), fontsize=20)\n",
        "        plt.title(f\"Graph for {tp_str} tp\", fontsize=20)\n",
        "        plt.savefig(f\"Graph_{tp_str}_{Coloring}_{Weight_type}.pdf\", format=\"pdf\", bbox_inches=\"tight\")\n",
        "        plt.show()\n",
        "    except Exception as e:\n",
        "      print(\"Error:\",e)\n",
        "######\n",
        "def show_name(change):\n",
        "    with index_output:\n",
        "        clear_output()\n",
        "        try:\n",
        "          idx = index_input.value\n",
        "          Class_info = np.load(\"Class_info.npy\",allow_pickle=True)\n",
        "          feature_name = Class_info[:,3]\n",
        "          print(f\"{idx}: {feature_name[idx]}\")\n",
        "        except Exception as e:\n",
        "            print(\"Error:\", e)\n",
        "######\n",
        "def show_plot(change):\n",
        "    with Plot_output:\n",
        "        clear_output()\n",
        "        try:\n",
        "          current_plot = Plot_option.value\n",
        "          if current_plot in ['Histogram: Correlation', 'Histogram: Probability', 'Histogram: Logistic']:\n",
        "            filename = current_plot.replace(\"Histogram: \",\"\")+\"_0.npy\"\n",
        "            CORR = np.load(filename, allow_pickle=True)\n",
        "            CORR = CORR.reshape(-1)\n",
        "            CORR = CORR[CORR > 0]\n",
        "            plt.hist(CORR, bins=np.size(CORR) // 100)\n",
        "            plt.show()\n",
        "          elif current_plot == \"DF and abundance\":\n",
        "            Data_full = np.load(\"Data_full.npy\", allow_pickle=True)\n",
        "            size = np.shape(Data_full)[0]\n",
        "            dim = np.shape(Data_full)[0]\n",
        "            detection_frequency = [np.mean(Data_full[:,i]>0) for i in range(dim)]\n",
        "            abundance =[np.mean(Data_full[i,:]>0) for i in range(size)]\n",
        "            fig, axes = plt.subplots(1, 2, figsize=(10, 5))\n",
        "            axes[0].boxplot(detection_frequency)\n",
        "            axes[0].set_title(\"Detection Frequency (for each feature)\")\n",
        "            axes[0].set_ylabel(\"Frequency\")\n",
        "            axes[0].tick_params(axis='x', which='both', bottom=False, top=False, labelbottom=False)\n",
        "            axes[1].boxplot(abundance)\n",
        "            axes[1].set_title(\"Abundance (for each sample)\")\n",
        "            axes[1].set_ylabel(\"\")\n",
        "            axes[1].tick_params(axis='x', which='both', bottom=False, top=False, labelbottom=False)\n",
        "            plt.tight_layout()\n",
        "            plt.show()\n",
        "          elif current_plot == \"Abundance per time point\":\n",
        "            Data_full = np.load(\"Data_full.npy\", allow_pickle=True)\n",
        "            size = np.shape(Data_full)[0]\n",
        "            timepoints = np.load(\"timepoints_full.npy\")\n",
        "            unique_tp = np.unique(timepoints)\n",
        "            abundance =np.array([np.mean(Data_full[i,:]>0) for i in range(size)])\n",
        "            abundance_tp = [abundance[timepoints == i] for i in unique_tp]\n",
        "            plt.figure(figsize=(8, 5))\n",
        "            plt.boxplot(abundance_tp, tick_labels=[str(i) for i in unique_tp])\n",
        "            plt.title(\"Abundance Grouped by Time Point\")\n",
        "            plt.xlabel(\"Time Point\")\n",
        "            plt.ylabel(\"Abundance\")\n",
        "            plt.tight_layout()\n",
        "            plt.show()\n",
        "        except Exception as e:\n",
        "            print(\"Error:\", e)\n",
        "######\n",
        "def plot_classifiction(b):\n",
        "  def evaluate_model(y_true, y_pred):\n",
        "    tn, fp, fn, tp = confusion_matrix(y_true, y_pred, labels=[1, 0]).ravel()\n",
        "    sensitivity = tp/(tp + fn)\n",
        "    specificity = tn / (tn + fp)\n",
        "    balanced_accuracy = (sensitivity + specificity) / 2\n",
        "    return balanced_accuracy, sensitivity, specificity\n",
        "  with class_output:\n",
        "    clear_output()\n",
        "    labels_list = []\n",
        "    try:\n",
        "      use_unanot = flag_unanot_class.value\n",
        "      val_type = class_selector.value\n",
        "      n_seeds = seeds_class.value\n",
        "      n_estimators = trees_class.value\n",
        "      n_splits = val_class.value\n",
        "      give_info = flag_info_class.value\n",
        "      scaler = StandardScaler()\n",
        "      filename = \"Data\"\n",
        "      if use_unanot:\n",
        "        filename += \"_unanot\"\n",
        "      Data = np.load(filename+\".npy\", allow_pickle=True)\n",
        "      dim = Data.shape[1]\n",
        "      if os.path.exists('participants.npy'):\n",
        "        participants = np.load('participants.npy', allow_pickle=True)\n",
        "        if os.path.exists('timepoints.npy') and val_type == 'Stratified by labels':\n",
        "          timepoints = np.load('timepoints.npy', allow_pickle=True)\n",
        "          labels_list+=[\"Participant\"]\n",
        "      if os.path.exists('Labels.npy') and os.path.exists('Label_names.npy'):\n",
        "        Label_names = list(np.load('Label_names.npy', allow_pickle=True))\n",
        "        Labels = np.load('Labels.npy', allow_pickle=True)\n",
        "        labels_list+=Label_names\n",
        "      if len(labels_list)==0:\n",
        "        print(\"There are no available labels. Run Data Extraction to obtain participants and labels files.\")\n",
        "      else:\n",
        "        dict_mean = {}\n",
        "        dict_std = {}\n",
        "        with warnings.catch_warnings(action=\"ignore\"):\n",
        "          for to_predict in labels_list:\n",
        "            print(\"Classifying\",to_predict)\n",
        "            list_mean = []\n",
        "            list_std = []\n",
        "            list_pval = []\n",
        "            list_sens = []\n",
        "            list_spec = []\n",
        "            importance_list = np.zeros((10,dim))\n",
        "            if to_predict==\"Participant\":\n",
        "              unique_tp = np.unique(timepoints)\n",
        "              for i in range(10):\n",
        "                n_features = 2 ** i\n",
        "                Results = []\n",
        "                for tp in unique_tp:\n",
        "                  val_idx = timepoints==tp\n",
        "                  train_idx = timepoints!=tp\n",
        "                  X_train, X_val = Data[train_idx], Data[val_idx]\n",
        "                  y_train, y_val = participants[train_idx], participants[val_idx]\n",
        "                  scaler.fit(X_train)\n",
        "                  X_train = scaler.transform(X_train)\n",
        "                  X_val = scaler.transform(X_val)\n",
        "                  selector = SelectKBest(score_func=f_classif, k=n_features).fit(pd.DataFrame(X_train), y_train)  # Select the k most important features\n",
        "                  Index = selector.get_support()\n",
        "                  X_train1, X_val1 = X_train[:,Index], X_val[:,Index]\n",
        "                  for seed in range(n_seeds):\n",
        "                      rf = RandomForestClassifier(n_estimators=100, random_state=seed, class_weight=\"balanced\")\n",
        "                      rf.fit(X_train1, y_train)\n",
        "                      importances = rf.feature_importances_.reshape(1,-1)\n",
        "                      importance_list[i,Index] = importance_list[i,Index] + importances\n",
        "                      y_pred = rf.predict(X_val1)\n",
        "                      acc = sum((y_pred-y_val)==0)/len(y_pred)\n",
        "                      Results.append(acc)\n",
        "                list_mean.append(np.mean(Results))\n",
        "                list_std.append(np.std(Results))\n",
        "                pvalue= ttest_1samp(Results, 0.5, alternative=\"greater\")\n",
        "                list_pval.append(pvalue.pvalue)\n",
        "            else:\n",
        "              if to_predict==\"Sex\":\n",
        "                Outcome = Labels[:,2]\n",
        "                y = np.zeros_like(Outcome)\n",
        "                y[Outcome==\"f\"]=1\n",
        "              elif to_predict==\"Smoking\":\n",
        "                  y = Labels[:, 3]\n",
        "                  y[y-y!=0]=0\n",
        "              elif to_predict == \"BMI\":\n",
        "                  Outcome = Labels[:, 1]\n",
        "                  m = np.median(Outcome)\n",
        "                  print(\"Median BMI:\", m)\n",
        "                  y = np.zeros_like(Outcome)\n",
        "                  y[Outcome>m] = 1\n",
        "              elif to_predict == \"Season\":\n",
        "                  Outcome = Labels[:, 5]\n",
        "                  y = np.zeros_like(Outcome)\n",
        "                  y[Outcome == \"Winter\"] = 1\n",
        "                  y[Outcome == \"Spring\"] = 1\n",
        "              elif to_predict == \"Age\":\n",
        "                  Outcome = Labels[:, 0]\n",
        "                  m = np.mean(Outcome)\n",
        "                  print(\"Median age:\", m)\n",
        "                  y = np.zeros_like(Outcome)\n",
        "                  y[Outcome >= m] = 1\n",
        "              elif to_predict == \"Visit\":\n",
        "                  Outcome = Labels[:, 4]\n",
        "                  m = np.mean(Outcome)\n",
        "                  y = np.zeros_like(Outcome)\n",
        "                  y[Outcome>=m] = 1\n",
        "              label_encoder = LabelEncoder()\n",
        "              y = label_encoder.fit_transform(y)\n",
        "              for i in range(10):\n",
        "                n_features = 2 ** i\n",
        "                Results = []\n",
        "                Results_sens = []\n",
        "                Results_spec = []\n",
        "                for seed in range(n_seeds):\n",
        "                  if val_type == 'Stratified by labels':\n",
        "                    skf = StratifiedKFold(n_splits=n_splits, shuffle=True, random_state=0)\n",
        "                    iterator = enumerate(skf.split(Data, y))\n",
        "                  else:\n",
        "                    skf = GroupKFold(n_splits=n_splits, shuffle=True, random_state=0)\n",
        "                    iterator = enumerate(skf.split(Data, y, groups=participants))\n",
        "                  for fold, (train_idx, val_idx) in iterator:\n",
        "                    X_train, X_val = Data[train_idx], Data[val_idx]\n",
        "                    y_train, y_val = y[train_idx], y[val_idx]\n",
        "                    selector = SelectKBest(score_func=f_classif, k=n_features).fit(pd.DataFrame(X_train), y_train)  # Select the k most important features\n",
        "                    Index = selector.get_support()\n",
        "                    X_train1, X_val1 = X_train[:,Index], X_val[:,Index]\n",
        "                    rf = RandomForestClassifier(n_estimators=100, random_state=seed, class_weight=\"balanced\")\n",
        "                    rf.fit(X_train1, y_train)\n",
        "                    importances = rf.feature_importances_.reshape(1,-1)\n",
        "                    importance_list[i,Index] = importance_list[i,Index] + importances\n",
        "                    y_pred = rf.predict(X_val1)\n",
        "                    acc, sensitivity, specificity = evaluate_model(y_val, y_pred)\n",
        "                    Results.append(acc)\n",
        "                    Results_sens.append(sensitivity)\n",
        "                    Results_spec.append(specificity)\n",
        "                list_mean.append(np.mean(Results))\n",
        "                list_std.append(np.std(Results))\n",
        "                list_sens.append(np.mean(Results_sens))\n",
        "                list_spec.append(np.mean(Results_spec))\n",
        "                pvalue= ttest_1samp(Results, 0.5, alternative=\"greater\")\n",
        "                list_pval.append(pvalue.pvalue)\n",
        "            dict_mean.update({to_predict: list_mean})\n",
        "            dict_std.update({to_predict: list_std})\n",
        "            if give_info:\n",
        "              arg_max = np.argmax(list_mean)\n",
        "              print(\"number of features:\", 2**arg_max)\n",
        "              print(\"mean:\", list_mean[arg_max])\n",
        "              print(\"std:\", list_std[arg_max])\n",
        "              if to_predict!=\"Participant\":\n",
        "                print(\"sensitivity:\", list_sens[arg_max])\n",
        "                print(\"specificity:\", list_spec[arg_max])\n",
        "              print(\"pvalue:\", list_pval[arg_max])\n",
        "              importance_row = importance_list[max(1, arg_max)]\n",
        "              print(\"features id:\", np.argsort(-importance_row)[:2])\n",
        "        num_features = 2 ** np.arange(10)\n",
        "        plt.figure(figsize=(8, 5))\n",
        "        for label in dict_mean:\n",
        "            mean = dict_mean[label]\n",
        "            std = dict_std[label]\n",
        "            plt.errorbar(num_features, mean, yerr=std, fmt='-', capsize=5, capthick=2, label=label)\n",
        "        plt.xscale(\"log\", base=2)\n",
        "        plt.xticks(num_features, labels=[f\"$2^{i}$\" for i in range(10)])\n",
        "        plt.xlabel(\"Number of Features (2^i)\")\n",
        "        plt.ylabel(\"AccuracyÂ± Std\")\n",
        "        plt.legend()\n",
        "        plt.grid(True, which=\"both\", linestyle=\"--\", linewidth=0.5)\n",
        "        #plt.savefig(f\"accuracy_vs_features_{val_type}.pdf\", format=\"pdf\", bbox_inches=\"tight\")\n",
        "        plt.show()\n",
        "    except Exception as e:\n",
        "      print(\"Error:\", e)\n",
        "###\n",
        "def plot_clusters(b):\n",
        "  with Cluster_output:\n",
        "    clear_output()\n",
        "    try:\n",
        "      print(\"Feauture selection is done by ranking ICC\")\n",
        "      np.random.seed(42)\n",
        "      n_seeds = seeds_cluster.value\n",
        "      Method = Cluster_option.value\n",
        "      Data_selection = Cluster_Data.value\n",
        "      filename = \"Data\"\n",
        "      Data = np.load(filename+\".npy\", allow_pickle=True)\n",
        "      ICC_info = np.load(\"ICC.npy\", allow_pickle=True)\n",
        "      if Data_selection != \"All classes\":\n",
        "        Class_info = np.load(\"Class_info.npy\", allow_pickle=True)\n",
        "        classes = Class_info[:,0]\n",
        "        Data = Data[:, classes == Data_selection]\n",
        "        ICC_info = ICC_info[classes == Data_selection]\n",
        "      y_train = np.load(\"participants.npy\")\n",
        "      n_participants = len(np.unique(y_train))\n",
        "      Data = Data.astype(float)\n",
        "      scaler = StandardScaler()\n",
        "      Data = scaler.fit_transform(Data)\n",
        "      Results_mean = []\n",
        "      Results_std = []\n",
        "      max_power = int(np.log2(np.shape(Data)[1]))\n",
        "      for f in range(1,max_power+1):\n",
        "        n_features = 2**f\n",
        "        Index = np.argsort(-ICC_info)[:n_features]\n",
        "        Data_indexed = Data[:, Index]\n",
        "        Results_k = []\n",
        "        for seed in range(n_seeds):\n",
        "          if Method == \"TSNE\":\n",
        "            X_train = TSNE(n_components=2, random_state=seed,n_jobs=1).fit_transform(Data_indexed)\n",
        "          elif Method == \"PCA\":\n",
        "            X_train = PCA(n_components=2).fit_transform(Data_indexed)\n",
        "          elif Method == \"UMAP\":\n",
        "            X_train = umap.UMAP(random_state=seed).fit_transform(Data_indexed)\n",
        "          kmeans = KMeans(n_clusters=n_participants, random_state=seed).fit(X_train)\n",
        "          clusters = kmeans.labels_\n",
        "          unique_count = 0\n",
        "          for i in range(n_participants):\n",
        "              class_i = y_train[clusters==i]\n",
        "              if len(np.unique(class_i))==1:\n",
        "                  id = class_i[0]\n",
        "                  if len(class_i)==  np.sum(y_train==id):\n",
        "                      unique_count+= 1\n",
        "          Results_k.append(unique_count/n_participants)\n",
        "        Results_mean.append(np.mean(Results_k))\n",
        "        Results_std.append(np.std(Results_k))\n",
        "      np.save(f\"Results_mean_{Method}\", Results_mean)\n",
        "      np.save(f\"Results_std_{Method}\", Results_std)\n",
        "      num_features = 2 ** np.arange(1,max_power+1)\n",
        "      plt.figure(figsize=(8, 5))\n",
        "      plt.errorbar(num_features, Results_mean, yerr=Results_std, fmt='-', capsize=5, capthick=2)\n",
        "      plt.xscale(\"log\", base=2)\n",
        "      plt.xticks(num_features, labels=[f\"$2^{i}$\" for i in range(1,max_power+1)])\n",
        "      plt.xlabel(\"Number of Features (2^i)\")\n",
        "      plt.ylabel(\"AccuracyÂ± Std\")\n",
        "      plt.title(f\"Accuracy of K-means with {Method}\")\n",
        "      plt.grid(True, which=\"both\", linestyle=\"--\", linewidth=0.5)\n",
        "      #plt.savefig(f\"Cluster_{Method}_{Data_selection}.pdf\", format=\"pdf\", bbox_inches=\"tight\")\n",
        "      plt.show()\n",
        "      print(\"Maximum value:\",np.max(Results_mean))\n",
        "      n_features = 2**(np.argmax(Results_mean)+1)\n",
        "      print(\"Number of stable features:\", n_features)\n",
        "      Index = np.argsort(-ICC_info)[:n_features]\n",
        "      Data_indexed = Data[:, Index]\n",
        "      if Method == \"TSNE\":\n",
        "        X_train = TSNE(n_components=2, random_state=seed,n_jobs=1).fit_transform(Data_indexed)\n",
        "      elif Method == \"PCA\":\n",
        "        X_train = PCA(n_components=2).fit_transform(Data_indexed)\n",
        "      elif Method == \"UMAP\":\n",
        "        X_train = umap.UMAP(random_state=seed).fit_transform(Data_indexed)\n",
        "      glasbey = cc.glasbey\n",
        "      cmap = ListedColormap(glasbey[:n_participants])\n",
        "      kmeans = KMeans(n_clusters=n_participants, random_state=seed).fit(X_train)\n",
        "      clusters = kmeans.labels_\n",
        "      plt.figure(figsize=(12, 6))\n",
        "      plt.subplot(1, 2, 1)\n",
        "      scatter = plt.scatter(X_train[:, 0], X_train[:, 1], c=clusters,cmap=cmap)\n",
        "      plt.title(\"KMeans Clusters\")\n",
        "      plt.xlabel(\"Component 1\")\n",
        "      plt.ylabel(\"Component 2\")\n",
        "      plt.subplot(1, 2, 2)\n",
        "      scatter = plt.scatter(X_train[:, 0], X_train[:, 1], c=y_train,cmap=cmap)\n",
        "      plt.title(\"Participants\")\n",
        "      plt.xlabel(\"Component 1\")\n",
        "      plt.ylabel(\"Component 2\")\n",
        "      plt.tight_layout()\n",
        "      #plt.savefig(f\"Kmeans_{Method}_{Data_selection}.pdf\", format=\"pdf\", bbox_inches=\"tight\")\n",
        "      plt.show()\n",
        "    except Exception as e:\n",
        "      print(\"Error:\", e)\n",
        "###\n",
        "def calculate_prediction_accuracy(b):\n",
        "  def forward_transform(y, eps=np.finfo(np.float64).eps):\n",
        "    return np.log(np.exp(y) - 1 + eps)\n",
        "  def inverse_transform(y_trans, eps = np.finfo(np.float64).eps):\n",
        "    return np.maximum(0, np.log(np.exp(y_trans) + 1 - eps))\n",
        "  def balanced_accuracy(y_true, y_pred):\n",
        "    tn, fp, fn, tp = confusion_matrix(y_true, y_pred, labels=[1, 0]).ravel()\n",
        "    sensitivity = tp/(tp + fn)\n",
        "    specificity = tn / (tn + fp)\n",
        "    ba = (sensitivity + specificity) / 2\n",
        "    return ba\n",
        "  def get_consecutive_pairs(X, timepoints, participants):\n",
        "    df_meta = pd.DataFrame({\n",
        "        'participant': participants,\n",
        "        'time': timepoints,\n",
        "        'order': np.arange(len(timepoints))\n",
        "    })\n",
        "    df_feat = pd.DataFrame(X, columns=[f'feat_{d}' for d in range(X.shape[1])])\n",
        "    df = pd.concat([df_meta, df_feat], axis=1)\n",
        "    df_t = df.copy()\n",
        "    df_tp1 = df.copy()\n",
        "    df_tp1['time'] -= 1  # Shift time back so merge finds (t, t+1)\n",
        "    merged = df_t.merge(\n",
        "        df_tp1,\n",
        "        on=['participant', 'time'],\n",
        "        suffixes=('_t1', '_t0')\n",
        "    )\n",
        "    D = X.shape[1]\n",
        "    X_t0 = merged[[f'feat_{d}_t0' for d in range(D)]].to_numpy()\n",
        "    X_t1 = merged[[f'feat_{d}_t1' for d in range(D)]].to_numpy()\n",
        "    return X_t0, X_t1, merged['order_t1'].to_numpy(), merged['time'].to_numpy()\n",
        "\n",
        "  def get_mean_other_samples(X, timepoints, participants):\n",
        "    df_meta = pd.DataFrame({\n",
        "        'participant': participants,\n",
        "        'time': timepoints,\n",
        "        'order': np.arange(len(timepoints))\n",
        "    })\n",
        "    df_feat = pd.DataFrame(X, columns=[f'feat_{d}' for d in range(X.shape[1])])\n",
        "    df = pd.concat([df_meta, df_feat], axis=1)\n",
        "    X_self = []\n",
        "    X_mean_others = []\n",
        "    orders = []\n",
        "    times = []\n",
        "    for participant, group in df.groupby('participant'):\n",
        "        if len(group) < 2:\n",
        "            continue  # Skip participants with fewer than 2 samples\n",
        "        features = group[[f'feat_{d}' for d in range(X.shape[1])]].to_numpy()\n",
        "        for i in range(len(group)):\n",
        "            x_i = features[i] # Current sample\n",
        "            other_idx = np.delete(np.arange(len(group)), i)\n",
        "            x_others_mean = features[other_idx].mean(axis=0) # Mean of all other samples\n",
        "            X_self.append(x_i)\n",
        "            X_mean_others.append(x_others_mean)\n",
        "            orders.append(group.iloc[i]['order'])\n",
        "            times.append(group.iloc[i]['time'])\n",
        "    return ( np.array(X_mean_others), np.array(X_self), np.array(orders), np.array(times)\n",
        "    )\n",
        "  with prediction_output:\n",
        "    clear_output()\n",
        "    try:\n",
        "      N_seeds = seeds_prediction.value\n",
        "      N_splits = val_prediction.value\n",
        "      Train_mode = not flag_Training.value\n",
        "      Method = Prediction_option.value\n",
        "      tp_difference = list(map(float, timepoints_input.value.split(',')))\n",
        "      Do_acc = flag_MSE.value\n",
        "      Training_selection = Prediction_radio.value\n",
        "      filter_ICC = flag_ICC.value\n",
        "      Exctract_features = flag_TSNE.value\n",
        "      only_imputation = flag_imputation.value\n",
        "      if only_imputation:\n",
        "        Data = np.load(\"Data_full.npy\", allow_pickle=True)\n",
        "        timepoints = np.load(\"timepoints_full.npy\")\n",
        "        participants = np.load(\"participants_full.npy\")\n",
        "        Outliers = np.load(\"Outliers.npy\")\n",
        "      else:\n",
        "        Data = np.load(\"Data.npy\", allow_pickle=True)\n",
        "        timepoints = np.load(\"timepoints.npy\")\n",
        "        participants = np.load(\"participants.npy\")\n",
        "      scaler = StandardScaler(with_mean=False)\n",
        "      Data = scaler.fit_transform(Data)\n",
        "      print(\"Normalization of variance is done.\")\n",
        "      #define kernel\n",
        "      if Method==\"RBF+Noise\":\n",
        "        kernel = RBF() + WhiteKernel(noise_level_bounds=(1e-12, 1))\n",
        "      elif Method==\"Matern\":\n",
        "        matern = Matern(length_scale=3,nu=3)\n",
        "        kernel = matern\n",
        "      else:\n",
        "        matern = Matern(length_scale=3,nu=3)\n",
        "        kernel = matern+WhiteKernel()\n",
        "      if Training_selection == \"Previous time point\":\n",
        "        X_t0, X_t1, order, time = get_consecutive_pairs(Data, timepoints, participants)\n",
        "      else:\n",
        "        X_t0, X_t1, order, time = get_mean_other_samples(Data, timepoints, participants)\n",
        "      if Method == \"two-way mixed\":\n",
        "        order = np.arange(len(timepoints))\n",
        "      order = order.astype(int)\n",
        "      testing_outlier = np.array([])\n",
        "      if only_imputation:\n",
        "        for Out in Outliers:\n",
        "          testing_outlier=np.append(testing_outlier, np.where(order==Out)[0])\n",
        "        testing_outlier = testing_outlier.astype(int)\n",
        "        training_outlier = np.setdiff1d(np.arange(len(order)),testing_outlier)\n",
        "      timepoints_former = time.astype(int)-1\n",
        "      timepoints_cont = [0]\n",
        "      timepoints_curr = 0\n",
        "      for tp in tp_difference:\n",
        "        timepoints_curr+=tp\n",
        "        timepoints_cont.append(timepoints_curr)\n",
        "      timepoints_onehot = np.array([timepoints_cont[i] for i in timepoints_former]).reshape(-1,1)\n",
        "      if filter_ICC:\n",
        "        ICC_info = np.load(\"ICC.npy\", allow_pickle=True)\n",
        "        n_features = 2**5\n",
        "        print(f\"{n_features} stable features are used\")\n",
        "        Index_ICC = np.argsort(-ICC_info)[:n_features]\n",
        "        X_t0 = X_t0[:,Index_ICC]\n",
        "      if Exctract_features:\n",
        "        print(\"Random seed for TSNE is fixed\")\n",
        "        X_t0 = TSNE(n_components=2, random_state=0).fit_transform(X_t0)\n",
        "      X_t0 = np.concatenate([X_t0,timepoints_onehot],axis=1)\n",
        "      all_preds = []\n",
        "      if Train_mode:\n",
        "        print(\"Training is started\")\n",
        "        for seed in range(N_seeds):\n",
        "            kf = KFold(n_splits=N_splits, shuffle=True, random_state=seed)\n",
        "            print(\"Seed:\", seed)\n",
        "            np.random.seed(seed)\n",
        "            if Method == 'CVAE':\n",
        "              model = CVAE(seed=seed)\n",
        "            elif Method == 'cGAN':\n",
        "              model = cGAN(seed=seed)\n",
        "            elif Method == \"Copying\":\n",
        "              model = None\n",
        "            elif Method == \"two-way mixed\":\n",
        "              with warnings.catch_warnings(action=\"ignore\"):\n",
        "                for fold_id, (train_idx, test_idx) in enumerate(kf.split(Data)):\n",
        "                  print(\"validation set n.\", fold_id)\n",
        "                  if only_imputation:\n",
        "                    train_idx = training_outlier\n",
        "                    test_idx = testing_outlier\n",
        "                  X_train = Data[train_idx]\n",
        "                  participants_train, participants_test= participants[train_idx], participants[test_idx]\n",
        "                  timepoints_train, timepoints_test = timepoints[train_idx], timepoints[test_idx]\n",
        "                  fold_pred = []\n",
        "                  for d in range(Data.shape[1]):\n",
        "                    scores_train = X_train[:, d]\n",
        "                    df_train = pd.DataFrame({\n",
        "                        'participant': participants_train,\n",
        "                        'time': timepoints_train,\n",
        "                        'value': scores_train\n",
        "                    })\n",
        "                    df_test = pd.DataFrame({\n",
        "                        'participant': participants_test,\n",
        "                        'time': timepoints_test,\n",
        "                    })\n",
        "                    model = mixedlm(\"value ~ time\", df_train, groups=df_train[\"participant\"])\n",
        "                    result = model.fit()\n",
        "                    y_d_pred =result.predict(df_test).tolist()\n",
        "                    fold_pred.append(y_d_pred)\n",
        "                  all_preds.append(fold_pred)\n",
        "                  if only_imputation:\n",
        "                    break\n",
        "            else:\n",
        "              model = GaussianProcessRegressor(random_state=seed,normalize_y=True, kernel = kernel)\n",
        "            if Method != \"two-way mixed\":\n",
        "              for _, (train_idx, test_idx) in enumerate(kf.split(X_t0)):\n",
        "                if only_imputation:\n",
        "                  train_idx = training_outlier\n",
        "                  test_idx = testing_outlier\n",
        "                X_train, X_test = X_t0[train_idx], X_t0[test_idx]\n",
        "                Y_train, Y_test = X_t1[train_idx], X_t1[test_idx]\n",
        "                if Method in ['GPR: RBF+Noise', 'GPR: Matern', 'GPR: Matern+Noise']:\n",
        "                  eps = np.min(Y_train[Y_train>0])/2\n",
        "                  Y_trans = forward_transform(Y_train, eps=eps)\n",
        "                  model.fit(X_train, Y_trans)\n",
        "                  Y_pred = model.predict(X_test)\n",
        "                  fold_pred = inverse_transform(Y_pred, eps=eps).tolist()\n",
        "                elif Method in ['CVAE','cGAN']:\n",
        "                  model.fit(X_train, Y_train)\n",
        "                  Y_pred = model.predict(X_test)\n",
        "                  fold_pred = Y_pred.tolist()\n",
        "                elif Method == \"Copying\":\n",
        "                  fold_pred = X_test[:,:-1].tolist()\n",
        "                all_preds.append(fold_pred)\n",
        "                if only_imputation:\n",
        "                  break\n",
        "        with open(f\"{Method}_{Training_selection}_{filter_ICC}_{Exctract_features}.json\", 'w') as f:\n",
        "          json.dump(all_preds, f)\n",
        "        print(\"Training is complete. The file is saved\")\n",
        "      k = 0\n",
        "      L2 = []\n",
        "      if Do_acc:\n",
        "        ba_sex,ba_bmi,ba_age = [],[],[]\n",
        "        if only_imputation:\n",
        "          Labels = np.load(\"Labels_full.npy\", allow_pickle=True)\n",
        "          Data_for_training, Labels_for_training = map(\n",
        "              lambda x: np.delete(x, Outliers, axis=0),\n",
        "              [Data, Labels]\n",
        "          )\n",
        "          eps = np.min(Data_for_training[Data_for_training>0])/2\n",
        "        else:\n",
        "          Labels = np.load(\"Labels.npy\", allow_pickle=True)\n",
        "          Data_for_training, Labels_for_training = Data, Labels\n",
        "        Label_names = np.load(\"Label_names.npy\", allow_pickle=True)\n",
        "        label_encoder = LabelEncoder()\n",
        "        ###sex\n",
        "        Outcome_sex = Labels[:,np.where(Label_names==\"Sex\")[0]]\n",
        "        y_sex = np.zeros_like(Outcome_sex)\n",
        "        y_sex[Outcome_sex==\"f\"]=1\n",
        "        y_sex = label_encoder.fit_transform(y_sex.ravel())\n",
        "        if only_imputation:\n",
        "          y_sex_for_training = np.delete(y_sex, Outliers, axis=0)\n",
        "        else:\n",
        "          y_sex_for_training = y_sex\n",
        "        y_sex_t1 = y_sex[order]\n",
        "        k_sex = 1\n",
        "        print(\"Number of features to predict sex is\",k_sex)\n",
        "        selector_sex = SelectKBest(score_func=f_classif, k=k_sex).fit(pd.DataFrame(Data_for_training), y_sex_for_training)\n",
        "        Index_sex = selector_sex.get_support()\n",
        "        ###BMI\n",
        "        Outcome_bmi = Labels[:, np.where(Label_names==\"BMI\")[0]]\n",
        "        m = np.median(Outcome_bmi)\n",
        "        y_bmi = np.zeros_like(Outcome_bmi)\n",
        "        y_bmi[Outcome_bmi>m] = 1\n",
        "        y_bmi = label_encoder.fit_transform(y_bmi.ravel())\n",
        "        if only_imputation:\n",
        "          y_bmi_for_training = np.delete(y_bmi, Outliers, axis=0)\n",
        "        else:\n",
        "          y_bmi_for_training = y_bmi\n",
        "        y_bmi_t1 = y_bmi[order]\n",
        "        k_bmi = 128\n",
        "        print(\"Number of features to predict BMI is\",k_bmi)\n",
        "        selector_bmi = SelectKBest(score_func=f_classif, k=k_bmi).fit(pd.DataFrame(Data_for_training), y_bmi_for_training)\n",
        "        Index_bmi = selector_bmi.get_support()\n",
        "        ###age\n",
        "        Outcome_age = Labels[:, np.where(Label_names==\"Age\")[0]]\n",
        "        m = np.mean(Outcome_age)\n",
        "        y_age = np.zeros_like(Outcome_age)\n",
        "        y_age[Outcome_age >= m] = 1\n",
        "        y_age = label_encoder.fit_transform(y_age.ravel())\n",
        "        if only_imputation:\n",
        "          y_age_for_training = np.delete(y_age, Outliers, axis=0)\n",
        "        else:\n",
        "          y_age_for_training = y_age\n",
        "        y_age_t1 = y_age[order]\n",
        "        k_age = 256\n",
        "        print(\"Number of features to predict age is\",k_age)\n",
        "        selector_age = SelectKBest(score_func=f_classif, k=k_age).fit(pd.DataFrame(Data_for_training), y_age_for_training)\n",
        "        Index_age = selector_age.get_support()\n",
        "      print(\"Testing is started\")\n",
        "      with open(f\"{Method}_{Training_selection}_{filter_ICC}_{Exctract_features}.json\", 'r') as f:\n",
        "          all_preds = json.load(f)\n",
        "          for seed in range(N_seeds):\n",
        "              print(\"Seed:\", seed)\n",
        "              np.random.seed(seed)\n",
        "              if Do_acc:\n",
        "                rf_sex = RandomForestClassifier(n_estimators=100, random_state=seed, class_weight=\"balanced\")\n",
        "                rf_sex.fit(Data_for_training[:,Index_sex], y_sex_for_training)\n",
        "                ##\n",
        "                rf_bmi = RandomForestClassifier(n_estimators=100, random_state=seed, class_weight=\"balanced\")\n",
        "                rf_bmi.fit(Data_for_training[:, Index_bmi], y_bmi_for_training)\n",
        "                ##\n",
        "                rf_age = RandomForestClassifier(n_estimators=100, random_state=seed, class_weight=\"balanced\")\n",
        "                rf_age.fit(Data_for_training[:, Index_age], y_age_for_training)\n",
        "\n",
        "              kf = KFold(n_splits=N_splits, shuffle=True, random_state=seed)\n",
        "              if Method == \"two-way mixed\":\n",
        "                iterator = enumerate(kf.split(Data))\n",
        "              else:\n",
        "                iterator = enumerate(kf.split(X_t0))\n",
        "              for _, (_, test_idx) in iterator:\n",
        "                if only_imputation:\n",
        "                  test_idx = testing_outlier\n",
        "                if Method == \"two-way mixed\":\n",
        "                  Y_test = Data[test_idx]\n",
        "                  Y_pred = np.array(all_preds[k]).transpose()\n",
        "                else:\n",
        "                  Y_test = X_t1[test_idx]\n",
        "                  Y_pred = np.array(all_preds[k])\n",
        "                if only_imputation:\n",
        "                  L2.append(np.mean(Y_pred>eps))\n",
        "                else:\n",
        "                  L2.append(np.mean((Y_test - Y_pred) ** 2))\n",
        "                k += 1\n",
        "                if Do_acc:\n",
        "                  ###\n",
        "                  label_test_sex = y_sex_t1[test_idx]\n",
        "                  label_pred_sex = rf_sex.predict(Y_pred[:,Index_sex])\n",
        "                  if only_imputation:\n",
        "                    ba_sex.append(sum(label_test_sex==label_pred_sex)/len(label_pred_sex))\n",
        "                  else:\n",
        "                    ba_sex.append(balanced_accuracy(label_test_sex, label_pred_sex))\n",
        "                  ###\n",
        "                  label_test_bmi = y_bmi_t1[test_idx]\n",
        "                  label_pred_bmi = rf_bmi.predict(Y_pred[:, Index_bmi])\n",
        "                  if only_imputation:\n",
        "                    ba_bmi.append(sum(label_test_bmi==label_pred_bmi)/len(label_pred_bmi))\n",
        "                  else:\n",
        "                    ba_bmi.append(balanced_accuracy(label_test_bmi, label_pred_bmi))\n",
        "                  ###\n",
        "                  label_test_age = y_age_t1[test_idx]\n",
        "                  label_pred_age = rf_age.predict(Y_pred[:, Index_age])\n",
        "                  if only_imputation:\n",
        "                    ba_age.append(sum(label_test_age==label_pred_age)/len(label_pred_age))\n",
        "                  else:\n",
        "                    ba_age.append(balanced_accuracy(label_test_age, label_pred_age))\n",
        "                if only_imputation:\n",
        "                  break\n",
        "          print(\"Mean and std:\")\n",
        "          if only_imputation:\n",
        "            print(\"Abundance\")\n",
        "          else:\n",
        "            print(\"MSE:\")\n",
        "          print(np.mean(L2), np.std(L2))\n",
        "          if Do_acc:\n",
        "            if only_imputation:\n",
        "              print(\"accuracy:\")\n",
        "            else:\n",
        "              print(\"balanced accuracy:\")\n",
        "            print(\"Sex:\",np.mean(ba_sex), np.std(ba_sex))\n",
        "            print(\"BMI:\",np.mean(ba_bmi), np.std(ba_bmi))\n",
        "            print(\"Age:\",np.mean(ba_age), np.std(ba_age))\n",
        "    except Exception as e:\n",
        "      print(\"Error:\", e)"
      ],
      "metadata": {
        "id": "Sjd4AdB9snUJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**All widgets**"
      ],
      "metadata": {
        "id": "HJu8KHINd62f"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hU1gmtD5XhtU"
      },
      "outputs": [],
      "source": [
        "# --- Widgets for Tab 1 ---\n",
        "outliers_input = widgets.Text(\n",
        "    value='200', #the sample with the lowest abundance\n",
        "    description='Outliers separeted by comma:',\n",
        "    layout=widgets.Layout(width='50%'),\n",
        "    style={'description_width': '190px'}\n",
        ")\n",
        "include_unanot_checkbox = widgets.Checkbox(\n",
        "    value=False,\n",
        "    description='Include unannotated data'\n",
        ")\n",
        "extraction_button = widgets.Button(\n",
        "    description='Run Extraction',\n",
        "    button_style='success'\n",
        ")\n",
        "extraction_output = widgets.Output()\n",
        "# --- Widgets for Tab 2 ---\n",
        "show_button = widgets.Button(description=\"Show Table\", button_style='primary')\n",
        "output_table = widgets.Output()\n",
        "# --- Widgets for Tab 3 ---\n",
        "flag_unanot_metric = widgets.Checkbox(\n",
        "      value=False,\n",
        "      description='Include unannotated data'\n",
        "  )\n",
        "flag_tp_metric = widgets.Checkbox(\n",
        "      value=False,\n",
        "      description='Calculate metrics per time point'\n",
        "  )\n",
        "alpha_input = widgets.BoundedFloatText(\n",
        "      value=0.01,\n",
        "      min=0,\n",
        "      max=1.0,\n",
        "      step=0.01,\n",
        "      description='Significance level:',\n",
        "      layout=widgets.Layout(width='300px'),\n",
        "      style={'description_width': '150px'}\n",
        "  )\n",
        "\n",
        "metric_selector = widgets.SelectMultiple(\n",
        "    options=['Correlation', 'Probability', 'Logistic', 'ICC', 'DF'],\n",
        "    description='Metrics:',\n",
        "    layout=widgets.Layout(width='300px', height='120px')\n",
        ")\n",
        "calc_metrics_button = widgets.Button(description=\"Calculate Metrics\", button_style='success')\n",
        "metrics_output = widgets.Output()\n",
        "# --- Widgets for Tab 4 ---\n",
        "Color_flag = widgets.Checkbox(\n",
        "      value=False,\n",
        "      description='Random Color'\n",
        ")\n",
        "\n",
        "feature1_input = widgets.BoundedIntText(\n",
        "    value=0,\n",
        "    min=0,\n",
        "    max=10**6,\n",
        "    description='1st feature:',\n",
        "    layout=widgets.Layout(width='250px')\n",
        ")\n",
        "feature2_input = widgets.BoundedIntText(\n",
        "    value=1,\n",
        "    min=0,\n",
        "    max=10**6,\n",
        "    description='2nd feature:',\n",
        "    layout=widgets.Layout(width='250px')\n",
        ")\n",
        "fit_type_selector = widgets.RadioButtons(\n",
        "    options=['Power', 'Linear'],\n",
        "    description='Law:',\n",
        "    layout=widgets.Layout(width='200px')\n",
        ")\n",
        "scatter_button = widgets.Button(description=\"Scatter Plot\", button_style='primary')\n",
        "scatter_output = widgets.Output()\n",
        "# --- Widgets for Tab 5 ---\n",
        "threshold_input = widgets.BoundedFloatText(\n",
        "      value=0.9,\n",
        "      min=0,\n",
        "      max=100,\n",
        "      step=0.01,\n",
        "      description='Threshold for edge values:',\n",
        "      layout=widgets.Layout(width='300px'),\n",
        "      style={'description_width': '170px'}\n",
        "  )\n",
        "flag_unanot_graph = widgets.Checkbox(\n",
        "      value=False,\n",
        "      description='Include unannotated data'\n",
        ")\n",
        "number_edges = widgets.BoundedIntText(\n",
        "    value=1000,\n",
        "    min=1,\n",
        "    max=10**6,\n",
        "    description='Maximum edges per node:',\n",
        "    layout=widgets.Layout(width='300px'),\n",
        "    style={'description_width': '200px'}\n",
        ")\n",
        "seed_edges = widgets.BoundedIntText(\n",
        "    value=42,\n",
        "    min=0,\n",
        "    max=10**6,\n",
        "    description='Any integer to change visualization:',\n",
        "    layout=widgets.Layout(width='300px'),\n",
        "    style={'description_width': '230px'}\n",
        ")\n",
        "Graph_tp = widgets.Checkbox(\n",
        "      value=False,\n",
        "      description='Show by time point'\n",
        "  )\n",
        "Coloring_radio = widgets.RadioButtons(\n",
        "    options=['Class', 'ICC'],\n",
        "    description='coloring by:',\n",
        "    layout=widgets.Layout(width='200px')\n",
        ")\n",
        "Graph_metric  = widgets.Dropdown(\n",
        "    options=['Correlation', 'Probability', 'Logistic'],\n",
        "    value='Correlation',\n",
        "    description='Metric:',\n",
        "    layout=widgets.Layout(width='250px')\n",
        ")\n",
        "Graph_button = widgets.Button(description=\"Plot graph\", button_style='primary')\n",
        "graph_output = widgets.Output()\n",
        "index_input = widgets.IntText(\n",
        "    value=0,\n",
        "    description='Show feature name at index:',\n",
        "    min=0,\n",
        "    style={'description_width': '200px'}\n",
        ")\n",
        "index_output = widgets.Output()\n",
        "# --- Widgets for Tab 6 ---\n",
        "Plot_option  = widgets.Dropdown(\n",
        "    options=['Select','Histogram: Correlation', 'Histogram: Probability', 'Histogram: Logistic',\n",
        "             'DF and abundance', 'Abundance per time point'],\n",
        "    value='Select',\n",
        "    description='Plot:',\n",
        "    layout=widgets.Layout(width='250px')\n",
        ")\n",
        "Plot_output = widgets.Output()\n",
        "# --- Widgets for Tab 7 ---\n",
        "flag_unanot_class = widgets.Checkbox(\n",
        "      value=False,\n",
        "      description='Include unannotated data'\n",
        ")\n",
        "flag_info_class = widgets.Checkbox(\n",
        "      value=True,\n",
        "      description='Print out detailed info'\n",
        ")\n",
        "class_selector = widgets.RadioButtons(\n",
        "    options=['Stratified by labels', 'Grouped by subjects'],\n",
        "    description='Validation sets:',\n",
        "    layout=widgets.Layout(width='400px'),\n",
        "    style={'description_width': '200px'}\n",
        ")\n",
        "seeds_class = widgets.BoundedIntText(\n",
        "    value=5,\n",
        "    min=1,\n",
        "    max=100,\n",
        "    description='Number of rounds:',\n",
        "    layout=widgets.Layout(width='300px'),\n",
        "    style={'description_width': '230px'}\n",
        ")\n",
        "val_class = widgets.BoundedIntText(\n",
        "    value=5,\n",
        "    min=2,\n",
        "    max=10,\n",
        "    description='Number of validation sets:',\n",
        "    layout=widgets.Layout(width='300px'),\n",
        "    style={'description_width': '230px'}\n",
        ")\n",
        "trees_class = widgets.BoundedIntText(\n",
        "    value=100,\n",
        "    min=1,\n",
        "    max=1000,\n",
        "    description='Number of decision trees:',\n",
        "    layout=widgets.Layout(width='300px'),\n",
        "    style={'description_width': '230px'}\n",
        ")\n",
        "Classification_button = widgets.Button(description=\"Plot balanced accuracy\", button_style='primary', layout=widgets.Layout(width='200px'))\n",
        "class_output = widgets.Output()\n",
        "# --- Widgets for Tab 8 ---\n",
        "seeds_cluster = widgets.BoundedIntText(\n",
        "    value=5,\n",
        "    min=1,\n",
        "    max=100,\n",
        "    description='Number of rounds:',\n",
        "    layout=widgets.Layout(width='300px'),\n",
        "    style={'description_width': '230px'}\n",
        ")\n",
        "Cluster_option  = widgets.Dropdown(\n",
        "    options=['TSNE', 'UMAP', 'PCA'],\n",
        "    value='TSNE',\n",
        "    description='Method:',\n",
        "    layout=widgets.Layout(width='250px')\n",
        ")\n",
        "Cluster_Data  = widgets.Dropdown(\n",
        "    options=['All classes', 'endogenous', 'environmental'],\n",
        "    value='All classes',\n",
        "    description='Data:',\n",
        "    layout=widgets.Layout(width='250px')\n",
        ")\n",
        "Cluster_button = widgets.Button(description=\"Group participants\", button_style='primary', layout=widgets.Layout(width='200px'))\n",
        "Cluster_output = widgets.Output()\n",
        "# --- Widgets for Tab 9 ---\n",
        "timepoints_input = widgets.Text(\n",
        "    value='0.25,0.25,0.25,0.5,0.5', #3 months and then 6 months\n",
        "    description='Difference in time points (years):',\n",
        "    layout=widgets.Layout(width='50%'),\n",
        "    style={'description_width': '190px'}\n",
        ")\n",
        "\n",
        "seeds_prediction = widgets.BoundedIntText(\n",
        "    value=5,\n",
        "    min=1,\n",
        "    max=100,\n",
        "    description='Number of rounds:',\n",
        "    layout=widgets.Layout(width='300px'),\n",
        "    style={'description_width': '230px'}\n",
        ")\n",
        "val_prediction = widgets.BoundedIntText(\n",
        "    value=5,\n",
        "    min=2,\n",
        "    max=10,\n",
        "    description='Number of validation sets:',\n",
        "    layout=widgets.Layout(width='300px'),\n",
        "    style={'description_width': '230px'}\n",
        ")\n",
        "flag_MSE = widgets.Checkbox(\n",
        "      value=True,\n",
        "      description='Calculate classification accuracy'\n",
        ")\n",
        "flag_Training = widgets.Checkbox(\n",
        "      value=False,\n",
        "      description='Training was done already'\n",
        ")\n",
        "Prediction_option  = widgets.Dropdown(\n",
        "    options=['CVAE', 'cGAN','Copying', 'two-way mixed','GPR: RBF+Noise', 'GPR: Matern', 'GPR: Matern+Noise'],\n",
        "    value='GPR: Matern+Noise',\n",
        "    description='Model:',\n",
        "    layout=widgets.Layout(width='250px')\n",
        ")\n",
        "Prediction_radio = widgets.RadioButtons(\n",
        "    options=['Previous time point','Mean of rest time points'],\n",
        "    description='Training set:',\n",
        "    layout=widgets.Layout(width='400px'),\n",
        "    style={'description_width': '150px'}\n",
        ")\n",
        "\n",
        "flag_ICC = widgets.Checkbox(\n",
        "      value=True,\n",
        "      description='Select only stable features'\n",
        ")\n",
        "\n",
        "flag_TSNE = widgets.Checkbox(\n",
        "      value=True,\n",
        "      description='Extract 2D features by TSNE'\n",
        ")\n",
        "\n",
        "flag_imputation = widgets.Checkbox(\n",
        "      value=False,\n",
        "      description='Generate samples for outliers'\n",
        ")\n",
        "\n",
        "Prediction_button = widgets.Button(description=\"Calculate loss\", button_style='primary', layout=widgets.Layout(width='200px'))\n",
        "\n",
        "prediction_output = widgets.Output()\n",
        "\n",
        "def create_tabs():\n",
        "    # --- Tab Setup ---\n",
        "  tab0 = widgets.Output()\n",
        "  tab1 = widgets.Output()\n",
        "  tab2 = widgets.Output()\n",
        "  tab3 = widgets.Output()\n",
        "  tab4 = widgets.Output()\n",
        "  tab5 = widgets.Output()\n",
        "  tab6 = widgets.Output()\n",
        "  tab7 = widgets.Output()\n",
        "  tab8 = widgets.Output()\n",
        "  tab9 = widgets.Output()\n",
        "  tabs = widgets.Accordion(children=[tab0, tab1, tab2, tab3, tab4, tab5, tab6, tab7, tab8, tab9])\n",
        "  tabs.set_title(0, 'Start')\n",
        "  tabs.set_title(1, 'Data Extraction: Create numpy .npy arrays from provided files')\n",
        "  tabs.set_title(2, 'Numbered features: List descriptions for all annotated features')\n",
        "  tabs.set_title(3, 'Measures: Calculate ICC, DF, and co-exposure measures')\n",
        "  tabs.set_title(4, 'Scatter plots: Plot two features with best-fit curve')\n",
        "  tabs.set_title(5, 'Graphs: Make graphs with feaures connected by co-exposure measures')\n",
        "  tabs.set_title(6, 'Plots: Histograms, boxplots')\n",
        "  tabs.set_title(7, 'Supervised classification: Predict labels')\n",
        "  tabs.set_title(8, 'Unsupervised classification: Group participants')\n",
        "  tabs.set_title(9, 'Prediction: modelling samples')\n",
        "# --- Create tabs ---\n",
        "  with tab0:\n",
        "      tab0.clear_output()\n",
        "      display(widgets.HTML(\"<p><b>Welcome</b>. All tabs for the analysis is presented below.</p>\"))\n",
        "      display(widgets.HTML(\"<p>Use <b>Data Extraction</b> to save information about features, participants, time points, classes, and labels.</p>\" ))\n",
        "      display(widgets.HTML(\"<p>Use <b>Numbered features</b> to have a list of all annotated features.</p>\" ))\n",
        "      display(widgets.HTML(\"<p>Use <b>Metrics</b> to to calculate metrics for further analysis</p>\" ))\n",
        "      display(widgets.HTML(\"<p>If not original data are used, the only changes needed are the way of producing. npy arrays in data_extraction function.</p>\" ))\n",
        "      display(widgets.HTML(\"<p>If any widgets appear before this tab, rerun this cell of code.</p>\" ))\n",
        "  with tab1:\n",
        "      tab1.clear_output()\n",
        "      display(widgets.HTML(\"<p>Upload the files with metadata and features first.</p>\"))\n",
        "      display(widgets.VBox([outliers_input, include_unanot_checkbox, extraction_button,extraction_output]))\n",
        "      extraction_button.on_click(data_extraction)\n",
        "  with tab2:\n",
        "      tab2.clear_output()\n",
        "      display(show_button)\n",
        "      display(output_table)\n",
        "      show_button.on_click(show_features)\n",
        "  with tab3:\n",
        "    tab3.clear_output()\n",
        "    display(flag_unanot_metric)\n",
        "    display(flag_tp_metric)\n",
        "    display(alpha_input)\n",
        "    display(metric_selector)\n",
        "    display(calc_metrics_button)\n",
        "    display(metrics_output)\n",
        "    calc_metrics_button.on_click(calculate_metrics)\n",
        "  with tab4:\n",
        "    tab4.clear_output()\n",
        "    display(Color_flag)\n",
        "    display(widgets.HBox([feature1_input, feature2_input]))\n",
        "    display(fit_type_selector)\n",
        "    display(scatter_button)\n",
        "    display(scatter_output)\n",
        "    scatter_button.on_click(scatter_button_plot)\n",
        "  with tab5:\n",
        "    tab5.clear_output()\n",
        "    display(threshold_input)\n",
        "    display(number_edges)\n",
        "    display(Graph_tp)\n",
        "    display(Coloring_radio)\n",
        "    display(Graph_metric)\n",
        "    display(seed_edges)\n",
        "    display(index_input, index_output)\n",
        "    display(Graph_button)\n",
        "    display(graph_output)\n",
        "    Graph_button.on_click(show_graph)\n",
        "    index_input.observe(show_name, names='value')\n",
        "  with tab6:\n",
        "    tab6.clear_output()\n",
        "    display(Plot_option,Plot_output)\n",
        "    Plot_option.observe(show_plot, names='value')\n",
        "  with tab7:\n",
        "    tab7.clear_output()\n",
        "    display(flag_unanot_class)\n",
        "    display(flag_info_class)\n",
        "    display(class_selector)\n",
        "    display(val_class)\n",
        "    display(seeds_class)\n",
        "    display(trees_class)\n",
        "    display(Classification_button)\n",
        "    display(class_output)\n",
        "    Classification_button.on_click(plot_classifiction)\n",
        "  with tab8:\n",
        "    tab8.clear_output()\n",
        "    display(seeds_cluster)\n",
        "    display(Cluster_option)\n",
        "    display(Cluster_Data)\n",
        "    display(Cluster_button)\n",
        "    display(Cluster_output)\n",
        "    Cluster_button.on_click(plot_clusters)\n",
        "  with tab9:\n",
        "    tab9.clear_output()\n",
        "    display(timepoints_input)\n",
        "    display(seeds_prediction)\n",
        "    display(val_prediction)\n",
        "    display(flag_Training)\n",
        "    display(flag_MSE)\n",
        "    display(Prediction_option)\n",
        "    display(Prediction_radio)\n",
        "    display(flag_ICC)\n",
        "    display(flag_TSNE)\n",
        "    display(Prediction_button)\n",
        "    display(flag_imputation)\n",
        "    display(prediction_output)\n",
        "    Prediction_button.on_click(calculate_prediction_accuracy)\n",
        "  return tabs"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**The tool**"
      ],
      "metadata": {
        "id": "sc4VAJfCUknz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "tabs = create_tabs()\n",
        "display(tabs)"
      ],
      "metadata": {
        "id": "9YMSWWvqIsAn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "2ByWJQkXPXz9"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}